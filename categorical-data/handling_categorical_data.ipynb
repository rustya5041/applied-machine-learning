{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice assignment: Handling categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dZEp05rN8tN"
   },
   "source": [
    "In this programming assignment, you are going to work with the following dataset from Kaggle:\n",
    "\n",
    "https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "Suppose that a company working with Big Data and Data Science wants to hire data scientists among people who have successfully passed some courses conducted by the company. Many people sign up for their training. The company wants to focus of the candidates who really want to work for them after training. Information related to demographics, education, experience is provided by the candidates via a sign-up form.\n",
    "\n",
    "This dataset is designed to understand the factors that lead a person to leave current job which is useful for HR researches. Based on the provided data, you are going to predict whether a candidate is looking for a job change.\n",
    "\n",
    "The whole data is divided into train and test parts. Data contains several categorical features – they need to be encoded.\n",
    "\n",
    "## Feature description\n",
    "\n",
    "- `enrollee_id`: Unique ID for a candidate\n",
    "- `city`: City code\n",
    "- `city_development_index`: Developement index of the city (scaled)\n",
    "- `gender`: Gender of a candidate\n",
    "- `relevent_experience`: Relevant experience of a candidate\n",
    "- `enrolled_university`: Type of University course enrolled in if any\n",
    "- `education_level`: Education level of a candidate\n",
    "- `major_discipline`: Education major discipline of a candidate\n",
    "- `experience`: Candidate total experience in years\n",
    "- `company_size`: Number of employees in a current employer's company\n",
    "- `company_type`: Type of a current employer\n",
    "- `last_new_job`: Difference in years between previous and current jobs\n",
    "- `training_hours`: training hours completed\n",
    "- `target`: 0 – Not looking for job change, 1 – Looking for a job change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-riW7UN88QB0"
   },
   "source": [
    "## 1\n",
    "\n",
    "Before modifying the features, let's study our dataframe a bit. First, call `.dtypes` for `train` dataframe. \n",
    "\n",
    "**q1:** How many columns in `train` contain data types different from numbers (ints and floats)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TLmsIbke6RTE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "cat_cols = train.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "print(len(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZmIR0DeNj-4"
   },
   "source": [
    "## 2\n",
    "\n",
    "**q2:** How many unique caterogies does a feature `'city'` contain in the train data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w1OFKy0S6SdX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.city.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT4YJ6DaOjF7"
   },
   "source": [
    "## 3\n",
    "\n",
    "**q3:** How many features in train data contain missing values (NaN)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BaEwQjRg6Tj_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in train.isna().sum() if i > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxc_ZFXyQ2lc"
   },
   "source": [
    "## 4\n",
    "\n",
    "Some features have a relatively small number of missing values. For instance, `'experience'` has only 65 missing values in the train data. Replacing these missing values with a special category might introduce bias to the data. However, since the number of missing values is not so big, we might be OK with it.\n",
    "\n",
    "Replace missing values in `'experience'` feature with a category -1.\n",
    "\n",
    "**q4:** How many categories does this feature contain in the train data now?\n",
    "\n",
    "_(hint: you might want to use `fillna` function from `pandas` library in this task, but remember that it's not an in-place function by default. Or you can use `SimpleImputer` from `sklearn`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dWv8Npro6VA1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['experience'].fillna(-1, inplace=True)\n",
    "\n",
    "train.experience.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7G5iQ2ZVNB"
   },
   "source": [
    "## 5\n",
    "\n",
    "`'education_level'` is an example of an ordinal feature. Sure, we can define an order on it. For instance, `'High School'` is \"bigger\" than `'Primary School'`, and `'Phd'` is \"bigger\" than '`Graduate'`. We can encode this feature with integer numbers in a correct order.\n",
    "\n",
    "In this task, apply a correct mapping for the values of `'education_level'` feature. The mapping should be the following:\n",
    "\n",
    "* `'Primary School'` -> 0\n",
    "* `'High School'` -> 1\n",
    "* `'Graduate'` -> 2\n",
    "* `'Masters'` -> 3\n",
    "* `'Phd'` -> 4\n",
    "\n",
    "At the same time, impute missing values in `'education_level'` with a new category -1. So another part of the mapping would be:\n",
    "\n",
    "* `np.nan` -> -1\n",
    "\n",
    "**q5:** What will be the mean value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest TWO decimal places (e.g. 12.3456789 -> 12.35).\n",
    "\n",
    "_(hint: you might want to use `map` function from `pandas` in this task)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BKDebpQX6WL7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.061384278108362"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping\n",
    "map_education = {'Primary School': 0, 'High School': 1, 'Graduate': 2, 'Masters': 3, 'Phd': 4, np.nan: -1}\n",
    "\n",
    "# call map\n",
    "train['education_level'] = train['education_level'].map(map_education)\n",
    "\n",
    "np.mean(train.education_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAi9S-YBrOlY"
   },
   "source": [
    "## 6\n",
    "\n",
    "Feature `'relevent_experience'` is an example of a binary feature. You can also check that it has no missing values, which makes its encoding pretty easy.\n",
    "\n",
    "Encode this feature with the following mapping:\n",
    "\n",
    "*   `\"No relevent experience\"` -> 0\n",
    "*   `\"Has relevent experience\"` -> 1\n",
    "\n",
    "**q6:** What will be the mean value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest TWO decimal places (e.g. 12.3456789 -> 12.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l5uTrVl36XjM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7199081323728991"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_relevent_experience = {'No relevent experience': 0, 'Has relevent experience': 1}\n",
    "\n",
    "train['relevent_experience'] = train['relevent_experience'].map(map_relevent_experience)\n",
    "np.mean(train.relevent_experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv-zgcIDtzqR"
   },
   "source": [
    "## 7\n",
    "\n",
    "In our case, `'gender'` is an example of a nominal feature (notice that it is not a binary feature cause it contains three categories + NaNs). We will use One-Hot encoding to encode it. There are several options how to do it in practice: for example, to use `get_dummies` function from `pandas` or `OneHotEncoder` from `sklearn.preprocessing`. Here, we will go with the first option.\n",
    "\n",
    "If you want to encode a whole dataset with One-Hot encoding, you can directly pass it into the discussed methods. But here we want to encode only one feature. We suggest to do it in four steps:\n",
    "\n",
    "1. Obtain a One-Hot encoding dataframe for the feature `'gender'`: apply `pd.get_dummies` to this feature. As a result, you should obtain a dataframe with three features (three different categories for gender). Don't include `'NaN'` feature - it will already be included as the encoding (0, 0, 0).\n",
    "2. Change the column names of this dataframe, so that a feature `'<category_name>'` will become `'gender-<category_name>'`. Of course, it is not a necessary step in general. However, it probably would be more convenient to work with a full dataframe if we remember that these One-Hot encoded features originally came from the feature `'gender'`. On a technical side, you can perform it by changing `df_gender.columns` list.\n",
    "3. Concatenate original and new dataframes. You can do it by calling `pd.concat` function. Don't forget to set `'axis'` parameter, so that you concatenate dataframes by columns, not by rows. As a result of this step, you should obtain a new `train` dataframe with three new columns named like `'gender-<category_name>'`.\n",
    "4. Finally, drop `'gender'` feature because we have already encoded it.\n",
    "\n",
    "As a result of these four steps, you should obtain a new version of `train` dataframe, with dropped `'gender'` feature and three new features named like `'gender-<category_name>'`. A total number of columns by this time should be equal to 16.\n",
    "\n",
    "**q7:** What is the total number of zero values which appear in the columns starting with `'gender-'`?\n",
    "\n",
    "_Example: suppose that you obtain the following dataframe:_\n",
    "\n",
    "| gender-category1 | gender-category2   | gender-category3|\n",
    "|------|------|------|\n",
    "|   0  | 1| 0|\n",
    "|0|0|0|\n",
    "\n",
    "_Then the answer for the question above should be 5._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7IGy0CV96Y8Q"
   },
   "outputs": [],
   "source": [
    "# get dummies\n",
    "df_gender = pd.get_dummies(train['gender'], prefix='gender').astype(np.int8)\n",
    "\n",
    "# concat train and df_gender\n",
    "train = pd.concat([train, df_gender], axis=1)\n",
    "\n",
    "# drop original column\n",
    "train = train.drop(columns='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42824\n"
     ]
    }
   ],
   "source": [
    "# count all rows with zero values in gender columns\n",
    "\n",
    "l = ['Male', 'Female', 'Other']\n",
    "fl = ['gender_' + _ for _ in l]\n",
    "\n",
    "zeros_gender = 0\n",
    "\n",
    "for col in fl:\n",
    "    zeros_gender += len(train[train[col] == 0])\n",
    "print(zeros_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJx2G8ZQv_Hy"
   },
   "source": [
    "## 8\n",
    "\n",
    "Perform One-Hot encoding for the feature `'enrolled_university'`, using the similar procedure as in the previous task:\n",
    "\n",
    "1. Obtain a One-Hot encoding dataframe for `'enrolled_university'`.\n",
    "2. Rename its columns.\n",
    "3. Concatenate original and One-Hot encoding dataframes.\n",
    "4. Drop `'enrolled_university'` column.\n",
    "\n",
    "A total number of columns by this time should be equal to 18.\n",
    "\n",
    "**q8:** What is the total number of zero values which appear in the columns starting with `'enrolled_university-'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TrHkhli66aPP"
   },
   "outputs": [],
   "source": [
    "# ohe on enrolled_university feature\n",
    "df_enrolled_university = pd.get_dummies(train['enrolled_university'], prefix='enrolled_university').astype(np.int8)\n",
    "\n",
    "# concat train and df_enrolled_university\n",
    "train = pd.concat([train, df_enrolled_university], axis=1)\n",
    "\n",
    "# drop original column\n",
    "train = train.drop(columns='enrolled_university')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38702\n"
     ]
    }
   ],
   "source": [
    "uni_cols = [i for i in train.columns if 'enrolled_university' in i]\n",
    "\n",
    "zeros_uni = 0\n",
    "\n",
    "for col in uni_cols:\n",
    "    zeros_uni += len(train[train[col] == 0])\n",
    "print(zeros_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBervQFY9N7K"
   },
   "source": [
    "## 9\n",
    "\n",
    "Encode feature `'city'` using frequency encoding. You should map each category `'city_i'` to its count (a total number of observations in `train` with `city == 'city_i'`). Save this mapping, since later you would apply the same mapping to the test data.\n",
    "\n",
    "As a result of this task, feature `'city'` should be encoded with category counts in `train`. \n",
    "\n",
    "**q9:** What will be the mean value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Co-kiLuc6bO6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1709.79643"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freq_encode_city(df):\n",
    "    map_city = df.city.value_counts().to_dict()\n",
    "    df['city'] = df['city'].map(map_city)\n",
    "    return df['city']\n",
    "\n",
    "# encode\n",
    "train['city'] = freq_encode_city(train)\n",
    "\n",
    "np.round(np.mean(train.city), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksJw_j-M9XCC"
   },
   "source": [
    "## 10\n",
    "\n",
    "Encode feature `'last_new_job'` with target encoding with no modifications. First, impute missing values in this feature with a new category `'-1'`. Then, map each category of `'last_new_job'` to the mean target value of the observations with the corresponding category. Save this mapping, since later you would apply the same mapping to the test data.\n",
    "\n",
    "**q10:** What will be the maximum value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.35).\n",
    "\n",
    "_(hint: you might want to use `groupby` function from `pandas` in this task)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tmZgKA1_6caQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36407"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_last_new_job(df):\n",
    "    df['last_new_job'] = df['last_new_job'].fillna('-1')\n",
    "    map_last_new_job = df.groupby('last_new_job')['target'].mean().to_dict()\n",
    "    df['last_new_job'] = df['last_new_job'].map(map_last_new_job)\n",
    "    return df['last_new_job']\n",
    "\n",
    "# encode\n",
    "train['last_new_job'] = encode_last_new_job(train)\n",
    "\n",
    "np.round(np.max(train.last_new_job), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyIVTPEY9eLb"
   },
   "source": [
    "## 11\n",
    "\n",
    "Encode feature `'experience'` with M-estimate encoding. Map each category of `'experience'` according to the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{x_{ij}} = \\frac{\\text{target}\\left(j, x_{ij}\\right) + m\\times y_{\\text{mean}}}{\\text{count}\\left(j, x_{ij}\\right) + m}\\quad,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $x_{ij}$ is a category being encoded,\n",
    "* $\\hat{x_{ij}}$ is its corresponding M-estimate encoding value,\n",
    "* $\\text{count}\\left(j, x_{ij}\\right)$ is a total number of times $x_{ij}$ appeared in `train`,\n",
    "* $\\text{target}\\left(j, x_{ij}\\right)$ is a mean target value of the observations with the corresponding category,\n",
    "* $m$ is a parameter.\n",
    "\n",
    "In this task, set $m = 0.5$. \n",
    "\n",
    "**q11:** What will be the maximum value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45383"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# map\n",
    "map_experience = MEstimateEncoder(cols='experience', m=0.5)\n",
    "\n",
    "# encode\n",
    "train['experience'] = map_experience.fit_transform(train['experience'], train['target'])\n",
    "\n",
    "# res\n",
    "np.round(np.max(train.experience), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0eCGRxLLJ-Y"
   },
   "source": [
    "## 12\n",
    "\n",
    "Encode feature `'major_discipline'` with Leave-One-Out encoding. Remember that this technique is similar to target encoding, but here while computing the encoding for a particular observation, we exclude it from the target encoding formula. Therefore a category $x_{ij}$ for the $i$-th observation will be encoded according to the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{x_{ij}} = \\frac{\\text{target}\\left(j, x_{ij}\\right) - y_i}{\\text{count}\\left(j, x_{ij}\\right) - 1}\\quad,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\hat{x_{ij}}$ is its corresponding M-estimate encoding value,\n",
    "* $\\text{count}\\left(j, x_{ij}\\right)$ is a total number of times $x_{ij}$ appeared in `train`,\n",
    "* $\\text{target}\\left(j, x_{ij}\\right)$ is a mean target value of the observations with the corresponding category,\n",
    "* $y_i$ is a target value of the $i$-th observation\n",
    "\n",
    "For example, suppose that you have the following train data:\n",
    "\n",
    "|feature|target|\n",
    "|-|-|\n",
    "|A|1|\n",
    "|A|0|\n",
    "|B|1|\n",
    "|B|0|\n",
    "|A|0|\n",
    "\n",
    "Then you obtain the following Leave-One-Out encoding:\n",
    "\n",
    "|feature|feature_loo_encoded|\n",
    "|-|-|\n",
    "|A|0.0|\n",
    "|A|0.5|\n",
    "|B|0.0|\n",
    "|B|1.0|\n",
    "|A|0.5|\n",
    "\n",
    "It is very important to notice that in this method the same category can be encoded differently for different observations. Thus, after encoding the train part of data, you should create a mapping which will help to encode the test data. In order to do this, simply average train encoding values within each category to obtain the final encoding. For instance, suppose that you obtain the following train dataframe:\n",
    "\n",
    "|feature|feature_loo_encoded|\n",
    "|-|-|\n",
    "|A|0.2|\n",
    "|A|0.6|\n",
    "|B|0.3|\n",
    "|B|0.7|\n",
    "|A|0.4|\n",
    "\n",
    "Then, for the test data, you should obtain the following mapping:\n",
    "\n",
    "* A -> 0.4 (because 0.4 is a mean value of the encoded values for the category A)\n",
    "* B -> 0.5 (because 0.5 is a mean value of the encoded values for the category B)\n",
    "\n",
    "Don't impute any missing values in this task. After completing this task, drop the feature `'major_discipline'` and rename `'major_discipline_loo_encoded'` to `'major_discipline'`.\n",
    "\n",
    "**q12:** What will be the maximum value of the encoding values for `'major_discipline'` for the TEST data in the mapping described above? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sV6QygUw6fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26772"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders import LeaveOneOutEncoder\n",
    "\n",
    "map_major_discipline = LeaveOneOutEncoder(cols='major_discipline')\n",
    "\n",
    "# encode\n",
    "train['major_discipline_loo_encoded'] = map_major_discipline.fit_transform(train['major_discipline'], train['target'])\n",
    "\n",
    "# drop original column, rename encoded column\n",
    "train = train.drop(columns='major_discipline').rename(columns={'major_discipline_loo_encoded': 'major_discipline'})\n",
    "\n",
    "# fit to test\n",
    "np.round(np.max(map_major_discipline.transform(test['major_discipline'])), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQmH1y-d9qOW"
   },
   "source": [
    "## 13\n",
    "\n",
    "Encode feature `'company_size'` with Catboost encoding. The technique was described in the lecture. Here, for the sake of simplicity, let's use the implementation from `category_encoders` library.\n",
    "\n",
    "As you may remember, Catboost encoding depends on how the data is ordered. Normally, you should shuffle the data one time or even several times. In this task, we will assume that the data has already been shuffled, so you don't need to shuffle it again.\n",
    "\n",
    "Take `CatBoostEncoder` and use the default values for all its parameters, except `handle_missing` - set it to `'return_nan'` so that your encoder don't do anything with missing values. Fit it on the `'company_size'` (train data) and `'target'` and transform this feature. Save the encoder - it will be used later to transform this feature in test data.\n",
    "\n",
    "Don't impute any missing values in this task.\n",
    "\n",
    "**q13:** What will be the most popular value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PfmCVKRx6hHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24935"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from scipy.stats import mode\n",
    "\n",
    "# map to train\n",
    "map_company_size = CatBoostEncoder(cols='company_size', handle_missing='return_nan')\n",
    "\n",
    "# encode\n",
    "train['company_size'] = map_company_size.fit_transform(train['company_size'], train['target'])\n",
    "\n",
    "np.round(mode(train.company_size, nan_policy='omit')[0], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBE4sCLL9uV4"
   },
   "source": [
    "## 14\n",
    "\n",
    "Encode feature `'company_type'` with Weight of Evidence (WoE) encoding. A category $x_{ij}$ will be encoded according to the following formula:\n",
    "\n",
    "$$\n",
    "\\hat{x_{ij}} = \\ln\\left(\\frac{\\mathbb{P}\\left(x_{ij}\\mid y=1\\right)}{\\mathbb{P}\\left(x_{ij}\\mid y=0\\right)}\\right)\\quad.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(x_{ij}\\mid y=1\\right) = \\frac{\\text{count}\\left(y=1\\mid x_{ij}\\right)}{\\text{count}\\left(y=1\\right)}\n",
    "$$\n",
    "$$\n",
    "\\mathbb{P}\\left(x_{ij}\\mid y=0\\right) = \\frac{\\text{count}\\left(y=0\\mid x_{ij}\\right)}{\\text{count}\\left(y=0\\right)}\n",
    "$$\n",
    "\n",
    "The notation means the following:\n",
    "\n",
    "* $\\text{count}\\left(y=1\\mid x_{ij}\\right)$ denotes the number of observations with the category $x_{ij}$ where the target value is equal to $1$;\n",
    "* $\\text{count}\\left(y=0\\mid x_{ij}\\right)$ denotes the same but for the target value $0$;\n",
    "* $\\text{count}\\left(y=1\\right)$ denotes the number of observations with the target value equal to $1$;\n",
    "* $\\text{count}\\left(y=0\\right)$ denotes the same but for the target value $0$.\n",
    "\n",
    "\n",
    "For example, suppose that you have the following train data:\n",
    "\n",
    "|feature|target|\n",
    "|-|-|\n",
    "|A|1|\n",
    "|A|0|\n",
    "|B|1|\n",
    "|B|0|\n",
    "|A|0|\n",
    "\n",
    "Then you obtain the following WoE encoding mapping:\n",
    "\n",
    "* A -> $\\ln\\left(\\frac{\\frac{1}{2}}{\\frac{2}{3}}\\right) \\approx -0.288$ \n",
    "* B -> $\\ln\\left(\\frac{\\frac{1}{2}}{\\frac{1}{3}}\\right) \\approx 0.405$ \n",
    "\n",
    "Don't impute any missing values in this task.\n",
    "\n",
    "**q14:** What will be the most popular value of this feature in the train data after the encoding? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.40878"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders import WOEEncoder\n",
    "map_company_type = WOEEncoder(cols='company_type', regularization=0)\n",
    "\n",
    "train['company_type'] = map_company_type.fit_transform(train['company_type'], train['target'])\n",
    "\n",
    "np.round(np.median(train.company_type), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFwADkfU9wxD"
   },
   "source": [
    "## 15\n",
    "\n",
    "We have encoded all categorical features. Next, we drop `'enrollee_id'` because it is not a representative feature. After this, we split train part of data into the dataframe which contains only features (without target) and the target array.\n",
    "\n",
    "Before training the models, we should impute the remaining missing values. You might have noticed that we didn't impute missing values for the features `'major_discipline'`, `'company_size'` and `'company_type'`. This is because the number of missing values in these features is relatively big (you can check it yourself). In practice, you might just create a special category (like `'-1'`) for each of these features before the encoding. However, in this task, let's perform the imputation using KNN approach - where we impute missing values by looking at the similar observations.\n",
    "\n",
    "Import `KNNImputer` from `sklearn`. It works only with the dataset with numbers - this is why we didn't run it before the encoding. Set `n_neighbors=3`, and let other parameters have the default values. Fit it on the train dataframe with features, and then transform it. Notice that after the transformation we will obtain `numpy.array` - make `pandas.DataFrame` out of it with the same columns as before.\n",
    "\n",
    "Save the KNN imputer - you will need it to process the test data. Check that there are no missing values in the train data anymore.\n",
    "\n",
    "**q15:** What is the mean value of the `'company_size'` feature in the train data after the imputation? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1610996515882,
     "user": {
      "displayName": "Evgeny Kovalev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhapCcPAzYLJE2xqpH7xgS8qtR8bZrkCgbHm27oLg=s64",
      "userId": "07828702499945486090"
     },
     "user_tz": -180
    },
    "id": "eCz_Pr_xoa82",
    "outputId": "1457ef28-3783-488e-ea52-fc79f2e6f893"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19158, 16), (19158,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(['enrollee_id', 'target'], axis=1)\n",
    "y_train = train['target']\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YvfwLqvS6kKr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18226"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# apply imputer\n",
    "X_train_imp = knn_imputer.fit_transform(X_train)\n",
    "\n",
    "# make pandas dataframe\n",
    "X_train = pd.DataFrame(X_train_imp, columns=X_train.columns)\n",
    "\n",
    "np.round(np.mean(X_train.company_size), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2T2a78d90mJ"
   },
   "source": [
    "## 16\n",
    "\n",
    "Finally, train a Random Forest classifier from `sklearn` on the train data. Set `n_estimators=500`, `max_depth=8`, `random_state=13`, and let other parameters have the default values. Check feature importances. \n",
    "\n",
    "**q16:** What is the name of the most important feature for this model? Provide the name of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g4_1694F6l0Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7232224747048347, 'major_discipline'),\n",
       " (0.09259636011790308, 'city_development_index'),\n",
       " (0.071106090906331, 'education_level'),\n",
       " (0.0338978512331609, 'company_type'),\n",
       " (0.03003712000207121, 'city'),\n",
       " (0.011585915861990839, 'experience'),\n",
       " (0.007888260376452148, 'last_new_job'),\n",
       " (0.0063619633978227895, 'company_size'),\n",
       " (0.005568817828513686, 'training_hours'),\n",
       " (0.005442621002178778, 'enrolled_university_Full time course'),\n",
       " (0.005133107213602752, 'relevent_experience'),\n",
       " (0.004732604057988538, 'enrolled_university_no_enrollment'),\n",
       " (0.0010335703963148064, 'gender_Male'),\n",
       " (0.0006705666869188496, 'gender_Female'),\n",
       " (0.0004487958150345902, 'enrolled_university_Part time course'),\n",
       " (0.000273880398881501, 'gender_Other')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=8, random_state=13)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rf.predict(X_train)\n",
    "\n",
    "# score\n",
    "train_score = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "# get feature importances and zip it with feature names\n",
    "feature_importances = sorted(zip(rf.feature_importances_, X_train.columns), reverse=True)\n",
    "feature_importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUydmtoi92d8"
   },
   "source": [
    "## 17\n",
    "\n",
    "In this last task, process the test data so that it is possible to make Random Forest predictions on for it. Perform the similar operations as for the train data, but remember that now you work with test observations and therefore all operations are in the inference mode.\n",
    "\n",
    "1. (task 4) Feature `'experience'`: impute missing values by -1. \n",
    "2. (task 5) Feature `'education_level'`: perform ordinal encoding mapping.\n",
    "3. (task 6) Feature `'relevent_experience'`: perform binary mapping.\n",
    "4. (task 7) Feature `'gender'`: perform One-Hot encoding and obtain three new features starting with `'gender-'`. Drop `'gender'` feature.\n",
    "5. (task 8) Feature `'enrolled_university'`: perform One-Hot encoding and obtain three new features starting with `'enrolled_university-'`. Drop `'enrolled_university'` feature.\n",
    "6. (task 9) Feature `'city'`: perform frequency encoding mapping.\n",
    "7. (task 10) Feature `'last_new_job'`: impute missing values by -1 and perform target encoding mapping.\n",
    "8. (task 11) Feature `'experience'`: perform M-estimate encoding mapping.\n",
    "9. (task 12) Feature `'major_discipline'`: perform Leave-One-Out encoding mapping.\n",
    "10. (task 13) Feature `'company_size'`: perform Catboost encoding mapping.\n",
    "11. (task 14) Feature `'company_type'`: perform WoE encoding mapping.\n",
    "12. (task 15) Split `test` into `X_test` (a dataframe with no `'enrollee_id'` and `'target'`) and `y_test` (an array with target values). Impute missing values by using KNN imputer which you used before (now only in a transform mode).\n",
    "\n",
    "As a result of the operations described above, you should obtain `X_test` which is a `pandas.DataFrame` with a shape (2129, 16). Calculate the predictions of the trained Random Forest model on it. Check the accuracy of the predictions on test data.\n",
    "\n",
    "Then calculate the predictions of the same model on `X_train` and check the accuracy there. Compare the accuracies on train and test data. Do you notice something? What, in your opinion, caused such difference in the accuracies?\n",
    "\n",
    "**q17:** As a result of this task, provide the accuracy for the test data, rounded to the nearest TWO decimal places (e.g. 12.3456789 -> 12.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "veC-y23Fwe8L"
   },
   "outputs": [],
   "source": [
    "# experience\n",
    "test['experience'] = test['experience'].fillna(-1)\n",
    "\n",
    "# education_level\n",
    "test['education_level'] = test['education_level'].map(map_education)\n",
    "\n",
    "# relevant experience\n",
    "test['relevent_experience'] = test['relevent_experience'].map(map_relevent_experience)\n",
    "\n",
    "# gender\n",
    "df_gender = pd.get_dummies(test['gender'], prefix='gender').astype(np.int8)\n",
    "test = pd.concat([test, df_gender], axis=1)\n",
    "test = test.drop(columns='gender')\n",
    "\n",
    "# enrolled_university\n",
    "df_enrolled_university = pd.get_dummies(test['enrolled_university'], prefix='enrolled_university').astype(np.int8)\n",
    "test = pd.concat([test, df_enrolled_university], axis=1)\n",
    "test = test.drop(columns='enrolled_university')\n",
    "\n",
    "# city\n",
    "test['city'] = freq_encode_city(test)\n",
    "\n",
    "# last_new_job\n",
    "test['last_new_job'] = test['last_new_job'].fillna('-1')\n",
    "test['last_new_job'] = encode_last_new_job(test)\n",
    "\n",
    "# experience\n",
    "test['experience'] = map_experience.transform(test['experience'])\n",
    "\n",
    "# major_discipline\n",
    "test['major_discipline'] = map_major_discipline.transform(test['major_discipline'])\n",
    "\n",
    "# company_size\n",
    "test['company_size'] = map_company_size.transform(test['company_size'])\n",
    "\n",
    "# company_type\n",
    "test['company_type'] = map_company_type.transform(test['company_type'])\n",
    "\n",
    "# split into X_test and y_test\n",
    "X_test = test.drop(['enrollee_id', 'target'], axis=1)\n",
    "y_test = test['target']\n",
    "\n",
    "# missing values\n",
    "X_test = X_test[X_train.columns]\n",
    "X_test_imp = knn_imputer.transform(X_test)\n",
    "\n",
    "# to pd\n",
    "X_test = pd.DataFrame(X_test_imp, columns=X_test.columns)\n",
    "\n",
    "assert X_test.shape == (2129, 16)\n",
    "assert X_test.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yM1CI29I6rv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.99\n",
      "Test score: 0.73\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# compare scores\n",
    "test_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Train score: {train_score:.2f}')\n",
    "print(f'Test score: {test_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO77HBmz1/PpQ55VCcYPEhx",
   "collapsed_sections": [],
   "name": "Week1_practice.ipynb",
   "provenance": [
    {
     "file_id": "1WztG2ZwKA0bx5LJIEQNQH23OULDBBmt8",
     "timestamp": 1610996564372
    }
   ]
  },
  "coursera": {
   "schema_names": [
    "categorical-data-task-week-1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
